{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Colab5-for-deeplearn.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nahilsobh/BBox-Label-Tool/blob/master/Colab5_for_deeplearn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "q9vG4323rfXm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's start with showing all the code from the previous lesson. Run this to train the model. Take note of the accuracy on the training and validation sets."
      ]
    },
    {
      "metadata": {
        "id": "MJPyDEzOqrKB",
        "colab_type": "code",
        "outputId": "ef6a8bbf-0b91-49ea-a127-613584c4e360",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1125
        }
      },
      "cell_type": "code",
      "source": [
        "# ----------------\n",
        "!wget --no-check-certificate \\\n",
        "        https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip \\\n",
        "      -O /tmp/cats_and_dogs_filtered.zip\n",
        "  \n",
        "# ----------------\n",
        "import os\n",
        "import zipfile\n",
        "\n",
        "local_zip = '/tmp/cats_and_dogs_filtered.zip'\n",
        "\n",
        "zip_ref   = zipfile.ZipFile(local_zip, 'r')\n",
        "\n",
        "zip_ref.extractall('/tmp')\n",
        "zip_ref.close()\n",
        "\n",
        "# ----------------\n",
        "base_dir = '/tmp/cats_and_dogs_filtered'\n",
        "\n",
        "train_dir      = os.path.join( base_dir, 'train'     )\n",
        "validation_dir = os.path.join( base_dir, 'validation')\n",
        "\n",
        "# Directory with our training cat/dog pictures\n",
        "train_cats_dir = os.path.join( train_dir, 'cats'     )\n",
        "train_dogs_dir = os.path.join( train_dir, 'dogs'     )\n",
        "\n",
        "# Directory with our validation cat/dog pictures\n",
        "validation_cats_dir = os.path.join( validation_dir, 'cats' )\n",
        "validation_dogs_dir = os.path.join( validation_dir, 'dogs' )\n",
        "\n",
        "# ----------------\n",
        "import tensorflow                           as     tf\n",
        "from   tensorflow.keras.optimizers          import RMSprop\n",
        "from   tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "model = tf.keras.models.Sequential([\n",
        "    # Note the input shape is the desired size of the image 150x150 with 3 bytes color\n",
        "    \n",
        "\n",
        "    tf.keras.layers.Conv2D      ( 16, (3,3), activation='relu', input_shape=(150, 150, 3)), # First convolution\n",
        "    tf.keras.layers.MaxPooling2D      (2,2)                       , # First  max pooling \n",
        "    \n",
        "    tf.keras.layers.Conv2D      ( 32, (3,3), activation='relu'   ), # Second convoution\n",
        "    tf.keras.layers.MaxPooling2D      (2,2),                        # Second max pooling \n",
        "    \n",
        "    tf.keras.layers.Conv2D      ( 64, (3,3), activation='relu'   ), # Third  convolution\n",
        "    tf.keras.layers.MaxPooling2D      (2,2)                       , # Third  max pooling\n",
        "    \n",
        "    tf.keras.layers.Flatten     ()                                , # Flatten the results to feed into a DNN\n",
        "    \n",
        "    tf.keras.layers.Dense       (512,        activation='relu'   ), # 512 neuron hidden layer\n",
        "    \n",
        "    tf.keras.layers.Dense       (  1,        activation='sigmoid')  #Only 1 output neuron. \n",
        "                                                                   # It will contain a value from 0-1 \n",
        "                                                                   # where 0 for 1 class ('cats') \n",
        "                                                                   # and 1 for the other ('dogs')\n",
        "])\n",
        "\n",
        "# ----------------\n",
        "model.compile( optimizer =   RMSprop(lr=0.001)    ,\n",
        "                    loss =  'binary_crossentropy' ,\n",
        "                 metrics = ['acc']\n",
        "             )\n",
        "\n",
        "# All images will be rescaled by 1./255.\n",
        "train_datagen = ImageDataGenerator( rescale = 1.0/255. )\n",
        "test_datagen  = ImageDataGenerator( rescale = 1.0/255. )\n",
        "\n",
        "# --------------------\n",
        "# Flow training images in batches of 20 using train_datagen generator\n",
        "# --------------------\n",
        "train_generator      = train_datagen.flow_from_directory( train_dir               , # directory for training images\n",
        "                                                          batch_size  = 20        ,\n",
        "                                                          class_mode  = 'binary'  , # binary labels to use with binary_crossentropy loss\n",
        "                                                          target_size = (150, 150)  # All images are resized to 150x150\n",
        "                                                         \n",
        "                                                        )     \n",
        "# --------------------\n",
        "# Flow validation images in batches of 20 using test_datagen generator\n",
        "# --------------------\n",
        "validation_generator =  test_datagen.flow_from_directory( validation_dir          ,\n",
        "                                                          batch_size  = 20        ,\n",
        "                                                          class_mode  = 'binary'  , \n",
        "                                                          target_size = (150, 150),\n",
        "                                                        )\n",
        "\n",
        "# --------------------\n",
        "history = model.fit_generator( train_generator                         ,\n",
        "                               validation_data  = validation_generator ,\n",
        "                               steps_per_epoch  = 100                  ,  # 2000 images = batch_size * steps\n",
        "                               epochs           =  15                  ,\n",
        "                               validation_steps =  50                  ,  # 1000 images = batch_size * steps\n",
        "                               verbose          =   2                     # Verbosity mode (one line per epoch)\n",
        "                             )"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-02-09 03:53:29--  https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 74.125.141.128, 2607:f8b0:400c:c06::80\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|74.125.141.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 68606236 (65M) [application/zip]\n",
            "Saving to: ‘/tmp/cats_and_dogs_filtered.zip’\n",
            "\n",
            "/tmp/cats_and_dogs_ 100%[===================>]  65.43M  91.7MB/s    in 0.7s    \n",
            "\n",
            "2019-02-09 03:53:30 (91.7 MB/s) - ‘/tmp/cats_and_dogs_filtered.zip’ saved [68606236/68606236]\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "Found 2000 images belonging to 2 classes.\n",
            "Found 1000 images belonging to 2 classes.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Epoch 1/15\n",
            "50/50 [==============================] - 27s 539ms/step - loss: 0.6947 - acc: 0.5690\n",
            " - 162s - loss: 1.2301 - acc: 0.5620 - val_loss: 0.6947 - val_acc: 0.5690\n",
            "Epoch 2/15\n",
            "50/50 [==============================] - 27s 547ms/step - loss: 0.5968 - acc: 0.6790\n",
            " - 164s - loss: 0.6380 - acc: 0.6525 - val_loss: 0.5968 - val_acc: 0.6790\n",
            "Epoch 3/15\n",
            "50/50 [==============================] - 26s 519ms/step - loss: 0.7055 - acc: 0.6470\n",
            " - 163s - loss: 0.5678 - acc: 0.7305 - val_loss: 0.7055 - val_acc: 0.6470\n",
            "Epoch 4/15\n",
            "50/50 [==============================] - 26s 522ms/step - loss: 0.6258 - acc: 0.7010\n",
            " - 161s - loss: 0.4618 - acc: 0.7815 - val_loss: 0.6258 - val_acc: 0.7010\n",
            "Epoch 5/15\n",
            "50/50 [==============================] - 27s 534ms/step - loss: 0.6521 - acc: 0.7070\n",
            " - 162s - loss: 0.3809 - acc: 0.8385 - val_loss: 0.6521 - val_acc: 0.7070\n",
            "Epoch 6/15\n",
            "50/50 [==============================] - 26s 521ms/step - loss: 0.6918 - acc: 0.7150\n",
            " - 162s - loss: 0.3079 - acc: 0.8675 - val_loss: 0.6918 - val_acc: 0.7150\n",
            "Epoch 7/15\n",
            "50/50 [==============================] - 27s 531ms/step - loss: 0.8764 - acc: 0.6850\n",
            " - 162s - loss: 0.2120 - acc: 0.9100 - val_loss: 0.8764 - val_acc: 0.6850\n",
            "Epoch 8/15\n",
            "50/50 [==============================] - 14s 283ms/step - loss: 1.2331 - acc: 0.6760\n",
            " - 101s - loss: 0.1445 - acc: 0.9525 - val_loss: 1.2331 - val_acc: 0.6760\n",
            "Epoch 9/15\n",
            "50/50 [==============================] - 14s 283ms/step - loss: 1.1192 - acc: 0.7050\n",
            " - 82s - loss: 0.1058 - acc: 0.9650 - val_loss: 1.1192 - val_acc: 0.7050\n",
            "Epoch 10/15\n",
            "50/50 [==============================] - 14s 281ms/step - loss: 1.5750 - acc: 0.6970\n",
            " - 82s - loss: 0.0741 - acc: 0.9730 - val_loss: 1.5750 - val_acc: 0.6970\n",
            "Epoch 11/15\n",
            "50/50 [==============================] - 14s 287ms/step - loss: 1.7328 - acc: 0.7070\n",
            " - 82s - loss: 0.0729 - acc: 0.9795 - val_loss: 1.7328 - val_acc: 0.7070\n",
            "Epoch 12/15\n",
            "50/50 [==============================] - 14s 285ms/step - loss: 1.9901 - acc: 0.6880\n",
            " - 83s - loss: 0.0992 - acc: 0.9820 - val_loss: 1.9901 - val_acc: 0.6880\n",
            "Epoch 13/15\n",
            "50/50 [==============================] - 14s 281ms/step - loss: 2.2000 - acc: 0.6760\n",
            " - 82s - loss: 0.0368 - acc: 0.9880 - val_loss: 2.2000 - val_acc: 0.6760\n",
            "Epoch 14/15\n",
            "50/50 [==============================] - 14s 284ms/step - loss: 2.1028 - acc: 0.6950\n",
            " - 82s - loss: 0.0318 - acc: 0.9885 - val_loss: 2.1028 - val_acc: 0.6950\n",
            "Epoch 15/15\n",
            "50/50 [==============================] - 14s 283ms/step - loss: 2.2700 - acc: 0.6700\n",
            " - 82s - loss: 0.0498 - acc: 0.9870 - val_loss: 2.2700 - val_acc: 0.6700\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "zb81GvNov-Tg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The Training Accuracy is about 98%, and the validation accuracy about 70%. This is a great example of overfitting -- which in short means that it can do very well with images it has seen before, but not so well with images it hasn't. Let's see if we can do better to avoid overfitting -- and one simple method is to augment the images a bit. If you think about it, most pictures of a cat are very similar -- the ears are at the top, then the eyes, then the mouth etc. Things like the distance between the eyes and ears will always be quite similar too. \n",
        "\n",
        "What if we tweak with the images to change this up a bit -- rotate the image, squash it, etc.  That's what image augementation is all about. And there's an API that makes it easy...\n",
        "\n",
        "Now take a look at the ImageGenerator. There are properties on it that you can use to augment the image. \n",
        "\n",
        "```\n",
        "# Updated to do image augmentation\n",
        "train_datagen = ImageDataGenerator(\n",
        "      rotation_range=40,\n",
        "      width_shift_range=0.2,\n",
        "      height_shift_range=0.2,\n",
        "      shear_range=0.2,\n",
        "      zoom_range=0.2,\n",
        "      horizontal_flip=True,\n",
        "      fill_mode='nearest')\n",
        "```\n",
        "These are just a few of the options available (for more, see the Keras documentation. Let's quickly go over what we just wrote:\n",
        "\n",
        "* rotation_range is a value in degrees (0–180), a range within which to randomly rotate pictures.\n",
        "* width_shift and height_shift are ranges (as a fraction of total width or height) within which to randomly translate pictures vertically or horizontally.\n",
        "* shear_range is for randomly applying shearing transformations.\n",
        "* zoom_range is for randomly zooming inside pictures.\n",
        "* horizontal_flip is for randomly flipping half of the images horizontally. This is relevant when there are no assumptions of horizontal assymmetry (e.g. real-world pictures).\n",
        "* fill_mode is the strategy used for filling in newly created pixels, which can appear after a rotation or a width/height shift.\n",
        "\n",
        "\n",
        "In the full listing below, find the ImageDataGenerator for the training data and replace it with one that uses Image Augmentation like above.\n",
        "Rerun the training...what's the impact? Don't forget to keep the normalization code. It's not in the above snippet, so add it as a parameter! :)\n"
      ]
    },
    {
      "metadata": {
        "id": "UK7_Fflgv8YC",
        "colab_type": "code",
        "outputId": "f42e4140-e792-4b14-ce4c-da989ea9365f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "cell_type": "code",
      "source": [
        "# ----------------\n",
        "!wget --no-check-certificate \\\n",
        "        https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip \\\n",
        "      -O /tmp/cats_and_dogs_filtered.zip\n",
        "  \n",
        "# ----------------\n",
        "import os\n",
        "import zipfile\n",
        "\n",
        "local_zip = '/tmp/cats_and_dogs_filtered.zip'\n",
        "\n",
        "zip_ref   = zipfile.ZipFile(local_zip, 'r')\n",
        "\n",
        "zip_ref.extractall('/tmp')\n",
        "zip_ref.close()\n",
        "\n",
        "# ----------------\n",
        "base_dir = '/tmp/cats_and_dogs_filtered'\n",
        "\n",
        "train_dir      = os.path.join( base_dir, 'train'     )\n",
        "validation_dir = os.path.join( base_dir, 'validation')\n",
        "\n",
        "# Directory with our training cat/dog pictures\n",
        "train_cats_dir = os.path.join( train_dir, 'cats'     )\n",
        "train_dogs_dir = os.path.join( train_dir, 'dogs'     )\n",
        "\n",
        "# Directory with our validation cat/dog pictures\n",
        "validation_cats_dir = os.path.join( validation_dir, 'cats' )\n",
        "validation_dogs_dir = os.path.join( validation_dir, 'dogs' )\n",
        "\n",
        "# ----------------\n",
        "import tensorflow                           as     tf\n",
        "from   tensorflow.keras.optimizers          import RMSprop\n",
        "from   tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "model = tf.keras.models.Sequential([\n",
        "    # Note the input shape is the desired size of the image 150x150 with 3 bytes color\n",
        "    \n",
        "\n",
        "    tf.keras.layers.Conv2D      ( 16, (3,3), activation='relu', input_shape=(150, 150, 3)), # First convolution\n",
        "    tf.keras.layers.MaxPooling2D      (2,2)                       , # First  max pooling \n",
        "    \n",
        "    tf.keras.layers.Conv2D      ( 32, (3,3), activation='relu'   ), # Second convoution\n",
        "    tf.keras.layers.MaxPooling2D      (2,2),                        # Second max pooling \n",
        "    \n",
        "    tf.keras.layers.Conv2D      ( 64, (3,3), activation='relu'   ), # Third  convolution\n",
        "    tf.keras.layers.MaxPooling2D      (2,2)                       , # Third  max pooling\n",
        "    \n",
        "    tf.keras.layers.Flatten     ()                                , # Flatten the results to feed into a DNN\n",
        "    \n",
        "    tf.keras.layers.Dense       (512,        activation='relu'   ), # 512 neuron hidden layer\n",
        "    \n",
        "    tf.keras.layers.Dense       (  1,        activation='sigmoid')  #Only 1 output neuron. \n",
        "                                                                   # It will contain a value from 0-1 \n",
        "                                                                   # where 0 for 1 class ('cats') \n",
        "                                                                   # and 1 for the other ('dogs')\n",
        "])\n",
        "\n",
        "# ----------------\n",
        "model.compile( optimizer =   RMSprop(lr=0.001)    ,\n",
        "                    loss =  'binary_crossentropy' ,\n",
        "                 metrics = ['acc']\n",
        "             )\n",
        "\n",
        "# All images will be rescaled by 1./255.\n",
        "train_datagen = ImageDataGenerator( rescale = 1.0/255. )\n",
        "test_datagen  = ImageDataGenerator( rescale = 1.0/255. )\n",
        "\n",
        "# --------------------\n",
        "# Flow training images in batches of 20 using train_datagen generator\n",
        "# --------------------\n",
        "train_generator      = train_datagen.flow_from_directory( train_dir               , # directory for training images\n",
        "                                                          batch_size  = 20        ,\n",
        "                                                          class_mode  = 'binary'  , # binary labels to use with binary_crossentropy loss\n",
        "                                                          target_size = (150, 150)  # All images are resized to 150x150\n",
        "                                                         \n",
        "                                                        )     \n",
        "# --------------------\n",
        "# Flow validation images in batches of 20 using test_datagen generator\n",
        "# --------------------\n",
        "validation_generator =  test_datagen.flow_from_directory( validation_dir          ,\n",
        "                                                          batch_size  = 20        ,\n",
        "                                                          class_mode  = 'binary'  , \n",
        "                                                          target_size = (150, 150),\n",
        "                                                        )\n",
        "\n",
        "# --------------------\n",
        "history = model.fit_generator( train_generator                         ,\n",
        "                               validation_data  = validation_generator ,\n",
        "                               steps_per_epoch  = 100                  ,  # 2000 images = batch_size * steps\n",
        "                               epochs           =  15                  ,\n",
        "                               validation_steps =  50                  ,  # 1000 images = batch_size * steps\n",
        "                               verbose          =   2                     # Verbosity mode (one line per epoch)\n",
        "                             )"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-02-09 06:12:42--  https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 74.125.141.128, 2607:f8b0:400c:c06::80\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|74.125.141.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 68606236 (65M) [application/zip]\n",
            "Saving to: ‘/tmp/cats_and_dogs_filtered.zip’\n",
            "\n",
            "/tmp/cats_and_dogs_ 100%[===================>]  65.43M   106MB/s    in 0.6s    \n",
            "\n",
            "2019-02-09 06:12:43 (106 MB/s) - ‘/tmp/cats_and_dogs_filtered.zip’ saved [68606236/68606236]\n",
            "\n",
            "Found 2000 images belonging to 2 classes.\n",
            "Found 1000 images belonging to 2 classes.\n",
            "50/50 [==============================] - 14s 279ms/step - loss: 0.6188 - acc: 0.6750\n",
            " - 81s - loss: 0.7731 - acc: 0.5860 - val_loss: 0.6188 - val_acc: 0.6750\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "VifDIjss4GVC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "You'll probably see something like accuracy on training of around 72% (.7195) and accuracy on validation of around 74% (.7350) Your values may differ due to the random nature of the augmentation.\n",
        "\n",
        "It also likely slowed down the training as all the image processing was also being done. \n",
        "\n",
        "Notice how the latter has likely gone up? That's because, thanks to all of the augmentation of images that was going on, a greater diversity of shapes and sizes was 'seen' by the model in the training process. This becomes a handy technique to avoid overfitting. Try it out, and experiment with different values for each of the parameters to see what the impact on the results might be.\n",
        "\n",
        "\n",
        "TODO\n",
        "\n",
        "Now let's talk dropouts and\n",
        "tf.keras.layers.Dropout(0.5),"
      ]
    },
    {
      "metadata": {
        "id": "BDQGwwag4OUh",
        "colab_type": "code",
        "outputId": "2b390d38-74c7-4693-e50a-31d364da72a7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "cell_type": "code",
      "source": [
        "# ----------------\n",
        "!wget --no-check-certificate \\\n",
        "        https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip \\\n",
        "      -O /tmp/cats_and_dogs_filtered.zip\n",
        "  \n",
        "# ----------------\n",
        "import os\n",
        "import zipfile\n",
        "\n",
        "local_zip = '/tmp/cats_and_dogs_filtered.zip'\n",
        "\n",
        "zip_ref   = zipfile.ZipFile(local_zip, 'r')\n",
        "\n",
        "zip_ref.extractall('/tmp')\n",
        "zip_ref.close()\n",
        "\n",
        "# ----------------\n",
        "base_dir = '/tmp/cats_and_dogs_filtered'\n",
        "\n",
        "train_dir      = os.path.join( base_dir, 'train'     )\n",
        "validation_dir = os.path.join( base_dir, 'validation')\n",
        "\n",
        "# Directory with our training cat/dog pictures\n",
        "train_cats_dir = os.path.join( train_dir, 'cats'     )\n",
        "train_dogs_dir = os.path.join( train_dir, 'dogs'     )\n",
        "\n",
        "# Directory with our validation cat/dog pictures\n",
        "validation_cats_dir = os.path.join( validation_dir, 'cats' )\n",
        "validation_dogs_dir = os.path.join( validation_dir, 'dogs' )\n",
        "\n",
        "# ----------------\n",
        "import tensorflow                           as     tf\n",
        "from   tensorflow.keras.optimizers          import RMSprop\n",
        "from   tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "model = tf.keras.models.Sequential([\n",
        "    # Note the input shape is the desired size of the image 150x150 with 3 bytes color\n",
        "    \n",
        "\n",
        "    tf.keras.layers.Conv2D      ( 16, (3,3), activation='relu', input_shape=(150, 150, 3)), # First convolution\n",
        "    tf.keras.layers.MaxPooling2D      (2,2)                       , # First  max pooling \n",
        "    \n",
        "    tf.keras.layers.Conv2D      ( 32, (3,3), activation='relu'   ), # Second convoution\n",
        "    tf.keras.layers.MaxPooling2D      (2,2),                        # Second max pooling \n",
        "    \n",
        "    tf.keras.layers.Conv2D      ( 64, (3,3), activation='relu'   ), # Third  convolution\n",
        "    tf.keras.layers.MaxPooling2D      (2,2)                       , # Third  max pooling\n",
        "    \n",
        "    tf.keras.layers.Flatten     ()                                , # Flatten the results to feed into a DNN\n",
        "    \n",
        "    tf.keras.layers.Dense       (512,        activation='relu'   ), # 512 neuron hidden layer\n",
        "    \n",
        "    tf.keras.layers.Dense       (  1,        activation='sigmoid')  #Only 1 output neuron. \n",
        "                                                                   # It will contain a value from 0-1 \n",
        "                                                                   # where 0 for 1 class ('cats') \n",
        "                                                                   # and 1 for the other ('dogs')\n",
        "])\n",
        "\n",
        "# ----------------\n",
        "model.compile( optimizer =   RMSprop(lr=0.001)    ,\n",
        "                    loss =  'binary_crossentropy' ,\n",
        "                 metrics = ['acc']\n",
        "             )\n",
        "\n",
        "# All images will be rescaled by 1./255.\n",
        "train_datagen = ImageDataGenerator( rescale = 1.0/255. )\n",
        "test_datagen  = ImageDataGenerator( rescale = 1.0/255. )\n",
        "\n",
        "# --------------------\n",
        "# Flow training images in batches of 20 using train_datagen generator\n",
        "# --------------------\n",
        "train_generator      = train_datagen.flow_from_directory( train_dir               , # directory for training images\n",
        "                                                          batch_size  = 20        ,\n",
        "                                                          class_mode  = 'binary'  , # binary labels to use with binary_crossentropy loss\n",
        "                                                          target_size = (150, 150)  # All images are resized to 150x150\n",
        "                                                         \n",
        "                                                        )     \n",
        "# --------------------\n",
        "# Flow validation images in batches of 20 using test_datagen generator\n",
        "# --------------------\n",
        "validation_generator =  test_datagen.flow_from_directory( validation_dir          ,\n",
        "                                                          batch_size  = 20        ,\n",
        "                                                          class_mode  = 'binary'  , \n",
        "                                                          target_size = (150, 150),\n",
        "                                                        )\n",
        "\n",
        "# --------------------\n",
        "history = model.fit_generator( train_generator                         ,\n",
        "                               validation_data  = validation_generator ,\n",
        "                               steps_per_epoch  = 100                  ,  # 2000 images = batch_size * steps\n",
        "                               epochs           =  15                  ,\n",
        "                               validation_steps =  50                  ,  # 1000 images = batch_size * steps\n",
        "                               verbose          =   2                     # Verbosity mode (one line per epoch)\n",
        "                             )"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-02-09 06:14:29--  https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 74.125.141.128, 2607:f8b0:400c:c06::80\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|74.125.141.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 68606236 (65M) [application/zip]\n",
            "Saving to: ‘/tmp/cats_and_dogs_filtered.zip’\n",
            "\n",
            "\r          /tmp/cats   0%[                    ]       0  --.-KB/s               \r         /tmp/cats_  61%[===========>        ]  40.22M   201MB/s               \r/tmp/cats_and_dogs_ 100%[===================>]  65.43M   223MB/s    in 0.3s    \n",
            "\n",
            "2019-02-09 06:14:30 (223 MB/s) - ‘/tmp/cats_and_dogs_filtered.zip’ saved [68606236/68606236]\n",
            "\n",
            "Found 2000 images belonging to 2 classes.\n",
            "Found 1000 images belonging to 2 classes.\n",
            "50/50 [==============================] - 14s 289ms/step - loss: 0.6678 - acc: 0.6260\n",
            " - 83s - loss: 0.8189 - acc: 0.5435 - val_loss: 0.6678 - val_acc: 0.6260\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "dm-Srz8KTZvh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}